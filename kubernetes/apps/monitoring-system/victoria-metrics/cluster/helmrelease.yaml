---
# yaml-language-server: $schema=https://kubernetes-schemas.noirprime.com/helm.toolkit.fluxcd.io/helmrelease_v2.json
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: &name victoria-metrics-cluster
spec:
  interval: 30m
  chartRef:
    kind: OCIRepository
    name: victoria-metrics-k8s-stack

  values:
    fullnameOverride: *name
    victoria-metrics-operator:
      enabled: false
    defaultDashboards:
      enabled: false
    defaultRules:
      rules:
        groups:
          etcd:
            create: false
          kubernetesSystemControllerManager:
            create: false
          kubernetesSystemScheduler:
            create: false

    # : vmsingle as prometheus
    # : http://vmsingle-victoria-metrics-cluster.monitoring-system.svc.cluster.local:8428
    vmsingle:
      spec:
        retentionPeriod: "14d"
        storage:
          resources:
            requests:
              storage: 50Gi
        route:
          enabled: true
          annotations:
            gethomepage.dev/enabled: "true"
            gethomepage.dev/icon: victoriametrics.png
            gethomepage.dev/name: VictoriaMetrics
            gethomepage.dev/group: Observability
          hostnames: ["metrics.noirprime.com"]
          parentRefs:
            - name: internal
              namespace: kube-system
              sectionName: https
          matches:
            - path:
                type: Exact
                value: /
          filters:
            - type: URLRewrite
              urlRewrite:
                path:
                  type: ReplaceFullPath
                  replaceFullPath: /vmui/

    # : vmalert
    # : http://vmalert-victoria-metrics-cluster.monitoring-system.svc.cluster.local:8080
    vmalert:
      spec:
        datasource:
          url: "http://vmauth-victoria-metrics-cluster.monitoring-system.svc.cluster.local:8427"

    # : vmagent
    # : http://vmagent-victoria-metrics-cluster.monitoring-system.svc.cluster.local:8428

    # : vmauth
    # : http://vmauth-victoria-metrics-cluster.monitoring-system.svc.cluster.local:8427
    # processing vlogs and vmetrics alerts at same altertmanager instance
    vmauth:
      enabled: true
      spec:
        unauthorizedUserAccessSpec:
          url_map:
            - src_paths:
                - "/api/v1/query.*"
              url_prefix: "http://vmagent-victoria-metrics-cluster.monitoring-system.svc.cluster.local:8428"
            - src_paths:
                - "/select/logsql/.*"
              url_prefix: "http://victoria-logs-server.monitoring-system.svc.cluster.local:9428"

    # : alertmanager
    # : http://vmalertmanager-victoria-metrics-cluster.monitoring-system.svc.cluster.local:9093
    alertmanager:
      route:
        enabled: true
        annotations:
          gethomepage.dev/enabled: "true"
          gethomepage.dev/icon: alertmanager.png
          gethomepage.dev/name: AlertManager
          gethomepage.dev/group: Observability
        hostnames: ["alert.noirprime.com"]
        parentRefs:
          - name: internal
            namespace: kube-system
            sectionName: https
      useManagedConfig: true
      config:
        route:
          group_by: ["alertname", "job"]
          group_interval: 10m
          group_wait: 1m
          receiver: "default"
          repeat_interval: 12h
          routes:
            - receiver: "null"
              matchers:
                - alertname =~ "InfoInhibitor|Watchdog"
            - receiver: "default"
              matchers:
                - severity =~ "warning|critical"
        inhibitRules:
          - equal: ["alertname", "namespace"]
            sourceMatch:
              - severity = "critical"
            targetMatch:
              - severity = "warning"
        receivers:
          - name: "null"
          - name: "default"
            webhook_configs:
              # : apprise-webhook
              # - send_resolved: true
              #   url: http://webhook.monitoring-system.svc.cluster.local:8001?template=alertmanager
              - send_resolved: true
                url: https://robusta-runner.monitoring-system.svc.cluster.local/api/alerts
            pushoverConfigs:
              - sendResolved: true
                html: true
                message: |-
                  {{- range .Alerts }}
                    {{- if ne .Annotations.description "" }}
                      {{ .Annotations.description }}
                    {{- else if ne .Annotations.summary "" }}
                      {{ .Annotations.summary }}
                    {{- else if ne .Annotations.message "" }}
                      {{ .Annotations.message }}
                    {{- else }}
                      Alert description not available
                    {{- end }}
                    {{- if gt (len .Labels.SortedPairs) 0 }}
                      <small>
                        {{- range .Labels.SortedPairs }}
                          <b>{{ .Name }}:</b> {{ .Value }}
                        {{- end }}
                      </small>
                    {{- end }}
                  {{- end }}
                priority: |-
                  {{ if eq .Status "firing" }}1{{ else }}0{{ end }}
                sound: gamelan
                title: >-
                  [{{ .Status | toUpper }}{{ if eq .Status "firing" }}:{{ .Alerts.Firing | len }}{{ end }}]
                  {{ .CommonLabels.alertname }}
                ttl: 86400s
                token:
                  name: *name
                  key: PUSHOVER_TOKEN
                userKey:
                  name: *name
                  key: PUSHOVER_USER_KEY
                urlTitle: View in Alertmanager
                httpConfig:
                  proxyURL: "${HTTPS_PROXY}"
                  noProxy: "${NO_PROXY}"

    kubeApiServer:
      vmScrape:
        spec:
          metricRelabelConfigs:
            # Drop high cardinality labels
            - action: drop
              sourceLabels: [__name__]
              regex: (apiserver|etcd|rest_client)_request(|_sli|_slo)_duration_seconds_bucket
            - action: drop
              sourceLabels: [__name__]
              regex: (apiserver_response_sizes_bucket|apiserver_watch_events_sizes_bucket)

    kubelet:
      vmScrape:
        spec:
          metricRelabelConfigs:
            # Drop high-cardinality labels.
            - action: labeldrop
              regex: (uid|id|name)
            - action: drop
              sourceLabels: [__name__]
              regex: (rest_client_request_duration_seconds_bucket|rest_client_request_duration_seconds_sum|rest_client_request_duration_seconds_count)
            # Optional, Drop cgroup metrics with no pod.
            - sourceLabels: [id, pod]
              action: drop
              regex: ".+;"
            # Optional, Drop less useful container blkio metrics.
            - sourceLabels: [__name__]
              action: drop
              regex: "container_blkio_device_usage_total"
            # Optional, Drop less useful container filesystem io metrics.
            - sourceLabels: [__name__]
              action: drop
              regex: "container_fs_(io_current|io_time_seconds_total|io_time_weighted_seconds_total|reads_merged_total|sector_reads_total|sector_writes_total|writes_merged_total)"
            # Optional, Drop less useful container process metrics.
            - sourceLabels: [__name__]
              action: drop
              regex: "container_(tasks_state|threads_max)"

    # disable chart components
    grafana:
      enabled: false
    prometheus-node-exporter:
      enabled: false
    kube-state-metrics:
      enabled: false

    # disable not-used metrics scraping
    kubeControllerManager:
      enabled: false
    kubeEtcd:
      enabled: false
    kubeProxy:
      enabled: false
    kubeScheduler:
      enabled: false

    # vmalertmanager rules
    additionalVictoriaMetricsMap:
      vlogs-rules:
        groups:
          - name: vlogs.rules
            type: vlogs
            interval: 5m
            rules:
              - alert: HasErrorLog
                expr: 'env: "prod" AND status:~"error|warn" | stats by (service, kubernetes.pod) count() as errorLog | filter errorLog:>0'
                annotations:
                  description: 'Service {{$labels.service}} (pod {{ index $labels "kubernetes.pod" }}) generated {{$labels.errorLog}} error logs in the last 5 minutes'
                labels:
                  severity: critical
          - name: ServiceRequest
            type: vlogs
            interval: 5m
            rules:
              - alert: TooManyFailedRequest
                expr: '* | extract "ip=<ip> " | extract "status_code=<code>;" | stats by (ip) count() if (code:~4.*) as failed, count() as total| math failed / total as failed_percentage| filter failed_percentage :> 0.01 | fields ip,failed_percentage'
                annotations:
                  description: "Connection from address {{$labels.ip}} has {{$value}}% failed requests in last 5 minutes"
                labels:
                  severity: critical
      dockerhub-rules:
        groups:
          - name: dockerhub.rules
            rules:
              - alert: DockerhubRateLimitRisk
                expr: count(time() - container_last_seen{image=~"(docker.io).*",container!=""} < 30) > 100
                annotations:
                  summary: Kubernetes cluster Dockerhub rate limit risk
                labels:
                  severity: critical
      oom-rules:
        groups:
          - name: oom.rules
            rules:
              - alert: OOMKilled
                expr: (kube_pod_container_status_restarts_total - kube_pod_container_status_restarts_total offset 10m >= 1) and ignoring (reason) min_over_time(kube_pod_container_status_last_terminated_reason{reason="OOMKilled"}[10m]) == 1
                annotations:
                  summary: Container {{ $labels.container }} in pod {{ $labels.namespace }}/{{ $labels.pod }} has been OOMKilled {{ $value }} times in the last 10 minutes.
                labels:
                  severity: critical
      zfs-rules:
        groups:
          - name: zfs.rules
            rules:
              - alert: ZFSUnexpectedPoolState
                expr: node_zfs_zpool_state{state!="online"} > 0
                for: 15m
                annotations:
                  summary: ZFS pool {{$labels.zpool}} on {{$labels.instance}} is in a unexpected state {{$labels.state}}
                labels:
                  severity: critical
