---
# yaml-language-server: $schema=https://kubernetes-schemas.noirprime.com/helm.toolkit.fluxcd.io/helmrelease_v2.json
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: &name ollama
spec:
  interval: 30m
  chartRef:
    kind: OCIRepository
    name: app-template
    namespace: gitops-system

  values:
    defaultPodOptions:
      hostUsers: false
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchExpressions:
                  - key: app.kubernetes.io/name
                    operator: In
                    values: [*name]
              topologyKey: "kubernetes.io/hostname"
    controllers:
      *name :
        type: statefulset
        replicas: 3
        annotations:
          reloader.stakater.com/auto: "true"
          config.homelab.ops/proxy: "https" # https_proxy needed
        statefulset:
          volumeClaimTemplates:
            - name: *name
              globalMounts:
                - path: /root/.ollama
              accessMode: "ReadWriteOnce"
              storageClass: ceph-block
              size: 50Gi
        initContainers:
          model-puller:
            image:
              repository: mirror.gcr.io/ollama/ollama
              tag: 0.13.5
            command:
              - "/bin/sh"
              - "-c"
              - |
                MODEL_DIR="/root/.ollama/models/manifests/registry.ollama.ai/library/qwen3"
                mkdir -p $MODEL_DIR
                if [ ! -f "$MODEL_DIR/8b-q4_K_M" ]; then
                  echo "Model tag '8b-q4_K_M' not found. Starting local server..."
                  ollama serve > /tmp/ollama-serve.log 2>&1 &
                  PID=$!
                  echo "Waiting for Ollama API to become available..."
                  tries=0
                  while true; do
                    if curl -s http://127.0.0.1:11434/api/tags > /dev/null 2>&1; then
                      echo "Ollama API is ready."
                      break
                    fi
                    if [ $tries -gt 45 ]; then
                      echo "Error: Timeout waiting for ollama serve."
                      echo "=== Server Logs ==="
                      cat /tmp/ollama-serve.log
                      echo "==================="
                      kill $PID 2>/dev/null
                      exit 1
                    fi
                    sleep 2
                    tries=$((tries+1))
                  done
                  echo "Ollama server is up. Pulling Qwen3 8B (Q4_K_M)..."
                  ollama pull qwen3:8b-q4_K_M 2>&1 | tee /tmp/ollama-pull.log
                  if [ $? -eq 0 ]; then
                    echo "Pull complete."
                  else
                    echo "Pull failed. Logs:"
                    cat /tmp/ollama-pull.log
                  fi
                  echo "Pull complete. Stopping local server..."
                  kill $PID 2>/dev/null
                  wait $PID 2>/dev/nul
                else
                  echo "Model qwen3:8b-q4_K_M already exists. Skipping."
                fi
            env:
              OLLAMA_HOST: "127.0.0.1:11434"
              HTTP_PROXY: "${HTTP_PROXY}"
              HTTPS_PROXY: "${HTTPS_PROXY}"
              NO_PROXY: "${NO_PROXY}"
        containers:
          app:
            image:
              repository: mirror.gcr.io/ollama/ollama
              tag: 0.13.5
            env:
              OLLAMA_CONTEXT_LENGTH: "8192"
              OLLAMA_FLASH_ATTENTION: "0"
              OLLAMA_MAX_LOADED_MODELS: "1"
              OLLAMA_NUM_PARALLEL: "1"
              OLLAMA_KEEP_ALIVE: "-1"
              OLLAMA_KV_CACHE_TYPE: "q8_0"
            probes:
              liveness:
                enabled: true
                custom: true
                spec:
                  httpGet:
                    path: /api/version
                    port: &port 11434
                  initialDelaySeconds: 10
                  periodSeconds: 20
              startup:
                enabled: true
                custom: true
                spec:
                  httpGet:
                    path: /api/version
                    port: *port
                  failureThreshold: 60
                  periodSeconds: 5
            resources:
              requests:
                cpu: 2000m
              limits:
                memory: 10Gi
            securityContext:
              allowPrivilegeEscalation: false
              capabilities: { drop: ["ALL"] }
              readOnlyRootFilesystem: true

    service:
      app:
        ports:
          http:
            port: *port

    persistence:
      tmpfs:
        type: emptyDir
        globalMounts:
          - path: /tmp
