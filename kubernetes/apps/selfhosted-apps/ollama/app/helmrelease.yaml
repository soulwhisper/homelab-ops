---
# yaml-language-server: $schema=https://kubernetes-schemas.noirprime.com/helm.toolkit.fluxcd.io/helmrelease_v2.json
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: &name ollama
spec:
  interval: 30m
  chartRef:
    kind: OCIRepository
    name: app-template
    namespace: gitops-system

  values:
    defaultPodOptions:
      hostUsers: false
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchExpressions:
                  - key: app.kubernetes.io/name
                    operator: In
                    values: [*name]
              topologyKey: "kubernetes.io/hostname"
    controllers:
      *name :
        type: statefulset
        replicas: 3
        annotations:
          reloader.stakater.com/auto: "true"
          config.homelab.ops/proxy: "https" # https_proxy needed
        statefulset:
          volumeClaimTemplates:
            - name: *name
              globalMounts:
                - path: /root/.ollama
              accessMode: "ReadWriteOnce"
              storageClass: ceph-block
              size: 50Gi
        initContainers:
          model-puller:
            image:
              repository: mirror.gcr.io/ollama/ollama
              tag: 0.13.5
            command:
              - "/bin/sh"
              - "-c"
              - |
                mkdir -p /root/.ollama/models/manifests/registry.ollama.ai/library/qwen3/
                if [ ! -f /root/.ollama/models/manifests/registry.ollama.ai/library/qwen3/8b-q4_K_M ]; then
                  echo "Model tag '8b-q4_K_M' not found. Starting server to pull..."
                  ollama serve &
                  PID=$!
                  while ! ollama list > /dev/null 2>&1; do sleep 1; done
                  echo "Pulling Qwen3 8B (Q4_K_M)..."
                  ollama pull qwen3:8b-q4_K_M;
                  kill $PID;
                else
                  echo "Model qwen3:8b-q4_K_M already exists. Skipping."
                fi
            env:
              HTTP_PROXY: "${HTTP_PROXY}"
              HTTPS_PROXY: "${HTTPS_PROXY}"
              NO_PROXY: "${NO_PROXY}"
        containers:
          app:
            image:
              repository: mirror.gcr.io/ollama/ollama
              tag: 0.13.5
            env:
              OLLAMA_CONTEXT_LENGTH: "8192"
              OLLAMA_FLASH_ATTENTION: "0"
              OLLAMA_MAX_LOADED_MODELS: "1"
              OLLAMA_NUM_PARALLEL: "1"
              OLLAMA_KEEP_ALIVE: "-1"
              OLLAMA_KV_CACHE_TYPE: "q8_0"
            probes:
              liveness:
                enabled: true
                custom: true
                spec:
                  httpGet:
                    path: /api/version
                    port: &port 11434
                  initialDelaySeconds: 10
                  periodSeconds: 20
              startup:
                enabled: true
                custom: true
                spec:
                  httpGet:
                    path: /api/version
                    port: *port
                  failureThreshold: 60
                  periodSeconds: 5
            resources:
              requests:
                cpu: 2000m
              limits:
                memory: 10Gi
            securityContext:
              allowPrivilegeEscalation: false
              capabilities: { drop: ["ALL"] }
              readOnlyRootFilesystem: true

    service:
      app:
        ports:
          http:
            port: *port

    persistence:
      tmpfs:
        type: emptyDir
        globalMounts:
          - path: /tmp
