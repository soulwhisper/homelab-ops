---
# yaml-language-server: $schema=https://kubernetes-schemas.noirprime.com/helm.toolkit.fluxcd.io/helmrelease_v2.json
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: &name ollama
spec:
  interval: 30m
  chartRef:
    kind: OCIRepository
    name: app-template
    namespace: gitops-system

  values:
    defaultPodOptions:
      hostUsers: false
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchExpressions:
                  - key: app.kubernetes.io/name
                    operator: In
                    values: [*name]
              topologyKey: "kubernetes.io/hostname"
    controllers:
      *name :
        type: StatefulSet
        replicas: 3
        statefulset:
          volumeClaimTemplates:
            - metadata:
                name: *name
              spec:
                accessModes: ["ReadWriteOnce"]
                storageClassName: "ceph-block"
                resources:
                  requests:
                    storage: 50Gi
        annotations:
          reloader.stakater.com/auto: "true"
          config.homelab.ops/proxy: "https" # https_proxy needed
        initContainers:
          model-puller:
            image:
              repository: mirror.gcr.io/ollama/ollama
              tag: 0.13.5
            command:
              - "/bin/sh"
              - "-c"
              - |
                mkdir -p /root/.ollama/models/manifests/registry.ollama.ai/library/qwen2.5/
                if [ ! -f /root/.ollama/models/manifests/registry.ollama.ai/library/qwen2.5/7b ]; then
                  echo "Model tag '7b' not found. Starting server to pull..."
                  ollama serve &
                  PID=$!
                  while ! ollama list > /dev/null 2>&1; do sleep 1; done
                  echo "Pulling Qwen2.5 7B..."
                  ollama pull qwen2.5:7b;
                  kill $PID;
                else
                  echo "Model qwen2.5:7b already exists. Skipping."
                fi
            env:
              OLLAMA_MODELS: "/root/.ollama/models"
              OLLAMA_HOST: "127.0.0.1:11434"
              HTTP_PROXY: "${HTTP_PROXY}"
              HTTPS_PROXY: "${HTTPS_PROXY}"
              NO_PROXY: "${NO_PROXY}"
        containers:
          app:
            image:
              repository: mirror.gcr.io/ollama/ollama
              tag: 0.13.5
            env:
              OLLAMA_MODELS: "/root/.ollama/models"
              OLLAMA_HOST: "0.0.0.0:11434"
              OLLAMA_FLASH_ATTENTION: "0"
              OLLAMA_MAX_LOADED_MODELS: "1"
              OLLAMA_KV_CACHE_TYPE: "q8_0"
              OLLAMA_KEEP_ALIVE: "-1"
              OLLAMA_NUM_PARALLEL: "2"
            volumeMounts:
              - name: *name
                mountPath: /root/.ollama
            startupProbe:
              httpGet:
                path: /api/version
                port: &port 11434
              failureThreshold: 60
              periodSeconds: 5
            livenessProbe:
              httpGet:
                path: /api/version
                port: *port
              initialDelaySeconds: 10
              periodSeconds: 20
            resources:
              requests:
                cpu: 2000m
              limits:
                memory: 12Gi
            securityContext:
              allowPrivilegeEscalation: false
              capabilities: { drop: ["ALL"] }
              readOnlyRootFilesystem: true

    service:
      app:
        ports:
          http:
            port: *port

    persistence:
      tmpfs:
        type: emptyDir
        globalMounts:
          - path: /tmp
