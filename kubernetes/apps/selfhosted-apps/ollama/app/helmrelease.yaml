---
# yaml-language-server: $schema=https://kubernetes-schemas.noirprime.com/helm.toolkit.fluxcd.io/helmrelease_v2.json
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: &name ollama
spec:
  interval: 30m
  chartRef:
    kind: OCIRepository
    name: app-template
    namespace: gitops-system

  values:
    defaultPodOptions:
      hostUsers: false
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchExpressions:
                  - key: app.kubernetes.io/name
                    operator: In
                    values: [*name]
              topologyKey: "kubernetes.io/hostname"
    controllers:
      *name :
        type: statefulset
        replicas: 3
        annotations:
          reloader.stakater.com/auto: "true"
          config.homelab.ops/proxy: "https" # https_proxy needed
        initContainers:
          model-puller:
            image:
              repository: mirror.gcr.io/ollama/ollama
              tag: 0.14.3
            command:
              - "/bin/sh"
              - "-c"
              - |
                MODEL_DIR="/root/.ollama/models/manifests/registry.ollama.ai/library/qwen3"
                mkdir -p $MODEL_DIR
                if [ ! -f "$MODEL_DIR/8b" ]; then
                  echo "Model tag '8b' not found. Starting local server..."
                  ollama serve > /tmp/ollama-serve.log 2>&1 &
                  PID=$!
                  echo "Waiting for Ollama API to become available..."
                  tries=0
                  while ! ollama list > /dev/null 2>&1; do
                    sleep 2
                    tries=$((tries+1))
                    if [ $tries -gt 30 ]; then
                      echo "Error: Timeout waiting for ollama serve. dumping logs:"
                      cat /tmp/ollama-serve.log
                      exit 1
                    fi
                  done
                  echo "Ollama server is up. Pulling Qwen3 8B (Q4_K_M)..."
                  ollama pull qwen3:8b
                  echo "Pull complete. Stopping local server..."
                  kill $PID
                else
                  echo "Model qwen3:8b already exists. Skipping."
                fi
            env:
              OLLAMA_HOST: "127.0.0.1:11434"
        containers:
          app:
            image:
              repository: mirror.gcr.io/ollama/ollama
              tag: 0.14.3
            env:
              OLLAMA_CONTEXT_LENGTH: "8192"
              OLLAMA_FLASH_ATTENTION: "0"
              OLLAMA_MAX_LOADED_MODELS: "1"
              OLLAMA_NUM_PARALLEL: "1"
              OLLAMA_KEEP_ALIVE: "-1"
              OLLAMA_KV_CACHE_TYPE: "q8_0"
            probes:
              liveness:
                enabled: true
                custom: true
                spec:
                  httpGet:
                    path: /api/version
                    port: &port 11434
                  initialDelaySeconds: 10
                  periodSeconds: 20
              startup:
                enabled: true
                custom: true
                spec:
                  httpGet:
                    path: /api/version
                    port: *port
                  failureThreshold: 60
                  periodSeconds: 5
            resources:
              requests:
                cpu: 2000m
              limits:
                memory: 10Gi
            securityContext:
              allowPrivilegeEscalation: false
              capabilities: { drop: ["ALL"] }
              readOnlyRootFilesystem: true

    service:
      app:
        type: LoadBalancer
        annotations:
          lbipam.cilium.io/ips: "10.10.0.140"
        ports:
          http:
            port: *port

    persistence:
      app:
        existingClaim: *name
        globalMounts:
          - path: /root/.ollama
      tmpfs:
        type: emptyDir
        globalMounts:
          - path: /tmp
