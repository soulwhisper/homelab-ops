---
# yaml-language-server: $schema=https://raw.githubusercontent.com/bjw-s-labs/helm-charts/main/charts/other/app-template/schemas/helmrelease-helm-v2.schema.json
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: &name llama-deepseek
spec:
  interval: 30m
  chartRef:
    kind: OCIRepository
    name: app-template
    namespace: gitops-system

  values:
    controllers:
      llama-deepseek:
        annotations:
          reloader.stakater.com/auto: "true"
        containers:
          app:
            image:
              repository: ghcr.io/ggml-org/llama.cpp
              tag: full
            args:
              # sampling
              - |
                --temp 0.6 \
                --top-k 40 \
                --top-p 0.85 \
                --repeat-penalty 1.15 \
                --repeat-last-n 512
            env:
              LLAMA_ARG_HOST: "127.0.0.1"
              LLAMA_ARG_PORT: &port 8000
              # LLAMA_ARG_ENDPOINT_METRICS: 1
              # https://huggingface.co/mradermacher/DeepSeek-R1-Distill-Qwen-32B-Uncensored-GGUF/blob/main/DeepSeek-R1-Distill-Qwen-32B-Uncensored.Q8_0.gguf
              LLAMA_ARG_MODEL: "/models/deepseek-r1-34b-uncensored.gguf"
              # optimization
              LLAMA_ARG_CTX_SIZE: 8192 # 8k context
              LLAMA_ARG_JINJA: 1
              LLAMA_ARG_N_GPU_LAYERS: 0 # pure cpu
              LLAMA_ARG_THREADS: 14 # 13900H physical cores
              LLAMA_ARG_UBATCH: 1024
            resources:
              requests:
                cpu: 20m
              limits:
                memory: 42Gi # Q6_K

    service:
      app:
        ports:
          http:
            port: *port

    persistence:
      media:
        existingClaim: models
        globalMounts:
          - path: /models
