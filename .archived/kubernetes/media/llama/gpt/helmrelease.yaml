---
# yaml-language-server: $schema=https://raw.githubusercontent.com/bjw-s-labs/helm-charts/main/charts/other/app-template/schemas/helmrelease-helm-v2.schema.json
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
  name: &name llama-gpt
spec:
  interval: 30m
  chartRef:
    kind: OCIRepository
    name: app-template
    namespace: gitops-system

  values:
    controllers:
      llama-gpt:
        annotations:
          reloader.stakater.com/auto: "true"
        containers:
          app:
            image:
              repository: ghcr.io/ggml-org/llama.cpp
              tag: full
            args:
              # sampling
              - |
                --temp 1.0 --top-k 0 --top-p 1.0
            env:
              LLAMA_ARG_HOST: "127.0.0.1"
              LLAMA_ARG_PORT: &port 8000
              # LLAMA_ARG_ENDPOINT_METRICS: 1
              # https://huggingface.co/unsloth/gpt-oss-20b-GGUF/blob/main/gpt-oss-20b-F16.gguf
              LLAMA_ARG_MODEL: "/models/gpt-oss-20b-F16.gguf"
              # optimization
              LLAMA_ARG_CTX_SIZE: 16384 # 16k context
              LLAMA_ARG_FLASH_ATTN: 1
              LLAMA_ARG_JINJA: 1
              LLAMA_ARG_N_GPU_LAYERS: 0 # pure cpu
              LLAMA_ARG_THREADS: 14 # 13900H physical cores
              LLAMA_ARG_UBATCH: 1024
            resources:
              requests:
                cpu: 20m
              limits:
                memory: 30Gi # gpt-oss 20b fp16 16k, add overhead

    service:
      app:
        ports:
          http:
            port: *port

    persistence:
      media:
        existingClaim: models
        globalMounts:
          - path: /models
